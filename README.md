# âš½ğŸ¤– PrediÃ§Ã£o de DireÃ§Ã£o de PÃªnaltis com Computer Vision e Deep Learning

![Python](https://img.shields.io/badge/Python-3.12-blue?logo=python)
![PyTorch](https://img.shields.io/badge/Framework-Computer%20Vision-orange)
![MediaPipe](https://img.shields.io/badge/MediaPipe-Pose%20Estimation-green)
![YOLO](https://img.shields.io/badge/YOLO-v8-red?logo=yolo)
![Accuracy](https://img.shields.io/badge/Accuracy-46.3%25-brightgreen)
![Status](https://img.shields.io/badge/Status-Completed-success)

---

## ğŸ“Œ VisÃ£o Geral

Este projeto implementa um sistema de **prediÃ§Ã£o em tempo real da direÃ§Ã£o de pÃªnaltis** usando tÃ©cnicas avanÃ§adas de **Computer Vision** e **Machine Learning**. O sistema analisa a postura corporal do jogador durante a cobranÃ§a e prediz se o chute serÃ¡ para a **esquerda**, **direita** ou **centro** do gol.

A soluÃ§Ã£o combina trÃªs tecnologias principais:
- **YOLOv8:** DetecÃ§Ã£o de pessoas no frame
- **MediaPipe Pose:** ExtraÃ§Ã£o de 33 pontos de landmarks da pose corporal
- **MLP Neural Network:** ClassificaÃ§Ã£o da direÃ§Ã£o baseada em features extraÃ­das

Todo o pipeline foi implementado com foco em **inferÃªncia em tempo real**, permitindo prediÃ§Ãµes durante a execuÃ§Ã£o do pÃªnalti com antecedÃªncia mÃ©dia de **0.3 segundos** antes do chute.

---

## ğŸ† Resultados

### MÃ©tricas de Performance

- âœ… **AcurÃ¡cia Global: 46.3%** (vs. baseline aleatÃ³rio de 33.3%)
- âœ… **Taxa de DecisÃ£o: 63.3%** (95 de 150 vÃ­deos)
- âœ… **Ganho sobre baseline: +13%**
- âœ… **AntecedÃªncia MÃ©dia: 0.3s** antes do chute
- âœ… **FPS MÃ©dio: 20.5** frames por segundo

### DistribuiÃ§Ã£o de PrediÃ§Ãµes

| DireÃ§Ã£o | PrecisÃ£o | Recall |
|---------|----------|---------|
| **Esquerda** | 71.6% | - |
| **Direita** | 28.4% | - |
| **Centro** | 0% | - |

> **Nota:** O modelo apresenta viÃ©s para classificaÃ§Ã£o Ã  esquerda e dificuldade em identificar chutes no centro devido ao desbalanceamento do dataset.

---

## ğŸ–¼ï¸ DemonstraÃ§Ã£o Visual

### Pipeline de Processamento

```
VÃ­deo Input â†’ YOLO DetecÃ§Ã£o â†’ MediaPipe Pose â†’ Feature Engineering â†’ MLP â†’ PrediÃ§Ã£o
```

### PrediÃ§Ã£o em Tempo Real

O sistema exibe:
- Bounding box do jogador detectado
- Skeleton pose overlay (33 landmarks)
- PrediÃ§Ã£o de direÃ§Ã£o com confidence score
- AntecedÃªncia temporal do chute

---

## ğŸ§  Arquitetura do Sistema

### 1. Pipeline de ExtraÃ§Ã£o de Dados

#### **get_data.py** - Processamento de VÃ­deos

**Componentes principais:**

- **PersonTracker:** Sistema de tracking multi-objeto com IoU
  ```python
  - Rastreamento persistente de IDs
  - IoU threshold: 0.3
  - Max age: 30 frames
  - SeleÃ§Ã£o do melhor track por hits e idade
  ```

- **KalmanBoxTracker:** Filtro de Kalman para suavizaÃ§Ã£o
  ```python
  - Estado: [x_center, y_center, vx, vy]
  - SuavizaÃ§Ã£o de bounding boxes
  - ReduÃ§Ã£o de jitter temporal
  ```

- **ValidaÃ§Ã£o de Pose:**
  ```python
  - MÃ­nimo 20 landmarks visÃ­veis
  - Visibilidade > 0.5
  - Key points (nariz, ombros, quadris) > 0.7
  ```

- **NormalizaÃ§Ã£o Espacial:**
  - Origem: ponto mÃ©dio entre quadris
  - Escala: distÃ¢ncia entre quadris
  - Coordenadas: (x, y, z) normalizadas

**SaÃ­da:** `pose_dataset.csv` com 99 features (33 landmarks Ã— 3 coordenadas)

### 2. Feature Engineering

#### **modeling.ipynb** - CriaÃ§Ã£o de Features AvanÃ§adas

**Features extraÃ­das (107 no total):**

1. **Coordenadas Normalizadas (99):** f_0 atÃ© f_98
   - 33 landmarks Ã— 3 coordenadas (x, y, z)

2. **Velocidades (6):**
   - Pulso direito: vx, vy, vz
   - Pulso esquerdo: vx, vy, vz

3. **Centroides (1):**
   - Centro de massa corporal

4. **Ã‚ngulos Articulares (1):**
   - Ã‚ngulo do joelho direito (quadril-joelho-tornozelo)

**Processamento:**
```python
- ConversÃ£o wide â†’ long format
- CÃ¡lculo de velocidades entre frames
- Ã‚ngulos usando produto vetorial
- NormalizaÃ§Ã£o com StandardScaler
```

### 3. Modelo de ClassificaÃ§Ã£o

#### **MLP Neural Network**

**Arquitetura:**
```
Input Layer (107 features)
    â†“
Hidden Layer 1 (128 neurons) + ReLU
    â†“
Hidden Layer 2 (64 neurons) + ReLU
    â†“
Hidden Layer 3 (32 neurons) + ReLU
    â†“
Output Layer (3 classes) + Softmax
```

**HiperparÃ¢metros otimizados:**
- Learning rate: 0.001
- Batch size: 32
- Alpha (L2): 0.0001
- Optimizer: Adam
- Epochs: 500 (early stopping)

**Tratamento de Desbalanceamento:**
- SMOTE para oversampling de classes minoritÃ¡rias
- ValidaÃ§Ã£o cruzada estratificada (5-fold)

**MÃ©tricas de Treinamento:**
```python
GridSearchCV com 72 combinaÃ§Ãµes
360 fits totais (5 folds Ã— 72 configs)
Melhor score CV: ~0.46
```

### 4. Sistema de InferÃªncia em Tempo Real

#### **predict_live.py** - PrediÃ§Ã£o Live

**Componentes:**

1. **Detector YOLO:**
   - Modelo: YOLOv8s
   - Input size: 320Ã—320 (otimizado para velocidade)
   - ConfianÃ§a mÃ­nima: 0.3

2. **Extrator MediaPipe:**
   - Modelo: pose_landmarker_heavy.task
   - Min detection confidence: 0.3
   - Num poses: 1

3. **SuavizaÃ§Ã£o Temporal:**
   ```python
   - Buffer de 15 frames (~0.5s em 30fps)
   - Threshold de confianÃ§a: 0.75
   - DecisÃ£o final por mÃ©dia mÃ³vel
   ```

4. **Sistema de DecisÃ£o:**
   ```python
   if len(buffer) >= MIN_FRAMES:
       avg_confidence = mean(last_15_predictions)
       if avg_confidence > THRESHOLD:
           MAKE_DECISION()
   ```

**VisualizaÃ§Ã£o:**
- Overlay de skeleton pose
- Probabilidades por classe
- Barra de confianÃ§a
- Timestamp da decisÃ£o

---

## ğŸ“Š Feature Engineering Detalhado

### NormalizaÃ§Ã£o Espacial

A normalizaÃ§Ã£o utiliza os quadris como referÃªncia:

```python
x0 = (hip_left.x + hip_right.x) / 2
y0 = (hip_left.y + hip_right.y) / 2
z0 = (hip_left.z + hip_right.z) / 2

scale = sqrt((hip_left - hip_right)Â²)

x_norm = (x - x0) / scale
y_norm = (y - y0) / scale
z_norm = (z - z0) / scale
```

**Vantagens:**
- InvariÃ¢ncia a escala e posiÃ§Ã£o
- Foco em movimentos relativos
- Robustez a diferentes distÃ¢ncias da cÃ¢mera

### CÃ¡lculo de Velocidades

Velocidade estimada por diferenÃ§a finita:

```python
velocity = (position_t - position_t-1) / Î”t
```

**Landmarks rastreados:**
- Pulso direito (wrist_right)
- Pulso esquerdo (wrist_left)

### Ã‚ngulos Articulares

Ã‚ngulo do joelho calculado por produto vetorial:

```python
v1 = hip - knee
v2 = ankle - knee

cos(Î¸) = (v1 Â· v2) / (||v1|| Ã— ||v2||)
Î¸ = arccos(cos(Î¸))
```

---

## âš™ï¸ Tecnologias Utilizadas

### Core Libraries
- **Python 3.12**
- **OpenCV** (Processamento de vÃ­deo)
- **MediaPipe** (Pose estimation)
- **Ultralytics YOLOv8** (Object detection)
- **scikit-learn** (Machine learning)
- **pandas** / **NumPy** (Data manipulation)
- **joblib** (Model persistence)

### TÃ©cnicas AvanÃ§adas
- **Kalman Filtering** (filterpy)
- **SMOTE** (imblearn)
- **Grid Search CV** (Hyperparameter tuning)
- **Stratified K-Fold** (Cross-validation)

### Visualization
- **Matplotlib** / **Seaborn**
- **PIL** (Text rendering)
- **tqdm** (Progress bars)

---

## ğŸ“ Estrutura do Projeto

```
penalty-prediction/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pose_dataset.csv          # Dataset bruto extraÃ­do
â”‚   â”œâ”€â”€ features_essenciais.csv   # Features engineered
â”‚   â””â”€â”€ results_experimento.csv   # Resultados de teste
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ yolov8s.pt                # Detector YOLO
â”‚   â”œâ”€â”€ pose_landmarker_heavy.task # MediaPipe Pose
â”‚   â”œâ”€â”€ mlp_best_model.pkl        # MLP treinado
â”‚   â”œâ”€â”€ scaler.pkl                # StandardScaler
â”‚   â””â”€â”€ label_encoder.pkl         # Label Encoder
â”‚
â”œâ”€â”€ cuts-penalty/                  # VÃ­deos de entrada
â”‚   â”œâ”€â”€ -left_01.mp4
â”‚   â”œâ”€â”€ -right_01.mp4
â”‚   â””â”€â”€ center_01.mp4
â”‚
â”œâ”€â”€ scrapping.py                   # Download de vÃ­deos (yt-dlp)
â”œâ”€â”€ get_data.py                    # ExtraÃ§Ã£o de poses
â”œâ”€â”€ modeling.ipynb                 # Training pipeline
â”œâ”€â”€ result_analyses.ipynb          # AnÃ¡lise de resultados
â”œâ”€â”€ predict_live.py                # InferÃªncia em tempo real
â””â”€â”€ README.md
```

---

## ğŸš€ Como Usar

### 1. InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone https://github.com/seu-usuario/penalty-prediction.git
cd penalty-prediction

# Instale dependÃªncias
pip install opencv-python mediapipe ultralytics scikit-learn pandas numpy joblib filterpy imbalanced-learn tqdm pillow yt-dlp
```

### 2. Download de VÃ­deos (Opcional)

```bash
python scrapping.py
```

### 3. ExtraÃ§Ã£o de Poses

```bash
# Processar vÃ­deos e extrair dataset
python get_data.py

# Output: data/pose_dataset.csv
```

**ConfiguraÃ§Ãµes disponÃ­veis:**
```python
extractor = PoseExtractor(
    yolo_model="models/yolov8s.pt",
    pose_model="models/pose_landmarker_heavy.task",
    frame_skip=1,           # Processar todos os frames
    use_tracking=True,      # Ativar tracking
    target_size=640         # ResoluÃ§Ã£o YOLO
)
```

### 4. Treinamento do Modelo

Abra `modeling.ipynb` no Jupyter e execute todas as cÃ©lulas:

```bash
jupyter notebook modeling.ipynb
```

**Processo:**
1. Load e anÃ¡lise exploratÃ³ria
2. Feature engineering
3. Train/test split (80/20)
4. Grid Search CV
5. Treinamento final
6. Salvamento dos modelos

### 5. PrediÃ§Ã£o em Tempo Real

```bash
# Usando vÃ­deo
python predict_live.py --video cuts-penalty/-left_01.mp4

# Usando webcam
python predict_live.py --webcam

# Com debug no terminal
python predict_live.py --video test.mp4 --debug
```

**SaÃ­da:**
- Janela com visualizaÃ§Ã£o em tempo real
- PrediÃ§Ãµes exibidas no frame
- Logs de confianÃ§a (se --debug)

---

## ğŸ“ˆ AnÃ¡lise de Resultados

### Experimento Completo

150 vÃ­deos de teste foram processados:

```python
# Carregar resultados
df_results = pd.read_csv("data/results_experimento.csv")

# MÃ©tricas
acurÃ¡cia = 0.463  # 46.3%
taxa_decisÃ£o = 0.633  # 63.3%
antecedÃªncia_mÃ©dia = 0.3  # segundos
```

### DistribuiÃ§Ã£o Real vs Predita

| Label | Real | Predito |
|-------|------|---------|
| Left | 36.8% | **71.6%** |
| Right | 52.6% | 28.4% |
| Center | 10.5% | 0% |

### Desafios Identificados

1. **Desbalanceamento de Classes:**
   - Centro com apenas 10.5% dos samples
   - SMOTE aplicado, mas insuficiente

2. **ViÃ©s de PrediÃ§Ã£o:**
   - Modelo favorece classificaÃ§Ã£o Ã  esquerda
   - Dificuldade em generalizar para centro

3. **Taxa de NÃ£o-DecisÃ£o:**
   - 36.7% dos casos sem decisÃ£o firme
   - Threshold de confianÃ§a conservador (0.75)

### Pontos Fortes

1. **AntecedÃªncia Temporal:**
   - MÃ©dia de 0.3s antes do chute
   - Suficiente para reaÃ§Ã£o humana

2. **Performance em Tempo Real:**
   - ~20 FPS em hardware comum
   - LatÃªncia aceitÃ¡vel para aplicaÃ§Ãµes prÃ¡ticas

3. **Robustez:**
   - Tracking multi-frame
   - Filtro de Kalman reduz noise
   - ValidaÃ§Ã£o de poses

---

## ğŸ”¬ Detalhes TÃ©cnicos

### Formato do Dataset

**pose_dataset.csv:**
```
| video | frame | label | timestamp_ms | f_0 | f_1 | ... | f_98 |
|-------|-------|-------|--------------|-----|-----|-----|------|
| video1| 14    | left  | 466          | 0.12| -0.5| ... | 0.33 |
```

**features_essenciais.csv:**
```
| vel_wrist_r_x | vel_wrist_r_y | ... | angulo_joelho | f_0 | ... | label |
|---------------|---------------|-----|---------------|-----|-----|-------|
| 0.045         | -0.123        | ... | 2.34          | 0.12| ... | left  |
```

### Sistema de Logging

```python
# ConfiguraÃ§Ã£o
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pose_extraction.log'),
        logging.StreamHandler()
    ]
)
```

**Logs incluem:**
- Progresso de processamento
- Poses vÃ¡lidas detectadas
- EstatÃ­sticas por vÃ­deo
- Checkpoints de salvamento

### Checkpoints AutomÃ¡ticos

Durante extraÃ§Ã£o de dados:
```python
# A cada 500 samples
if len(all_data) % 500 == 0:
    save_checkpoint(f"data/checkpoint_{len(all_data)}.csv")
```

---

## ğŸ¯ Melhorias Futuras

### Curto Prazo

1. **Balanceamento de Dataset:**
   - Coletar mais vÃ­deos de chutes no centro
   - Aplicar tÃ©cnicas de augmentation temporal

2. **Feature Engineering:**
   - Ã‚ngulos adicionais (tornozelo, quadril)
   - AceleraÃ§Ã£o (segunda derivada)
   - Features de assimetria corporal

3. **Arquitetura do Modelo:**
   - Experimentar LSTM/GRU para sequÃªncias temporais
   - AtenÃ§Ã£o temporal nos Ãºltimos N frames
   - Ensemble de modelos

### Longo Prazo

1. **Deep Learning End-to-End:**
   - CNN 3D diretamente nos frames
   - Spatial-Temporal Graph CNN
   - Transformer para sequÃªncias de poses

2. **Dataset Expandido:**
   - MÃºltiplos Ã¢ngulos de cÃ¢mera
   - Diferentes nÃ­veis de competiÃ§Ã£o
   - Dados de treino de goleiros

3. **AplicaÃ§Ã£o PrÃ¡tica:**
   - App mobile para anÃ¡lise em campo
   - Sistema de treinamento para goleiros
   - AnÃ¡lise estatÃ­stica de jogadores

---

## ğŸ“š ReferÃªncias TÃ©cnicas

### Papers

1. **YOLO:**
   - Redmon et al. "You Only Look Once: Unified, Real-Time Object Detection"
   
2. **MediaPipe:**
   - Bazarevsky et al. "BlazePose: On-device Real-time Body Pose tracking"

3. **Pose Estimation:**
   - Cao et al. "OpenPose: Realtime Multi-Person 2D Pose Estimation"

### Frameworks

- [Ultralytics YOLOv8](https://docs.ultralytics.com/)
- [MediaPipe](https://google.github.io/mediapipe/)
- [scikit-learn](https://scikit-learn.org/)

---

## ğŸ‘¤ Autor

**KauÃ£ Dias**  
Estudante de EstatÃ­stica e entusiasta de Computer Vision & Deep Learning

- ğŸ™ GitHub: [github.com/Kauadp](https://github.com/Kauadp)  
- ğŸ”— LinkedIn: [linkedin.com/in/kauad](https://www.linkedin.com/in/kauad/)

---

## ğŸ“ Contato

Para dÃºvidas, sugestÃµes ou colaboraÃ§Ãµes:

- ğŸ“§ Email: [kauadp1405@example.com]
- ğŸ’¬ Issues: [GitHub Issues](https://github.com/seu-usuario/penalty-prediction/issues)

---

<div align="center">

**âš½ Feito com paixÃ£o por futebol e tecnologia ğŸ¤–**

*"A melhor defesa Ã© prever o ataque"*

</div>